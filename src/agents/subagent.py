"""Sub-agent nodes for the PR-level hybrid multi-agent review graph."""

from __future__ import annotations

import json
import logging
import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List

import yaml
from langchain_core.messages import HumanMessage, SystemMessage

from models.custom_openai import CodeReviewChatOpenAI
from utils.diff_parser import DiffParser
from utils.linter_runner import run_eslint, run_gitleaks, run_pip_audit, run_ruff
from .state import InlineComment, LintFinding, PRReviewState

logger = logging.getLogger(__name__)

_PROMPTS: Dict[str, Any] = {}
try:
    _PROMPT_PATH = Path(__file__).resolve().parents[1] / "models" / "prompts.yml"
    if _PROMPT_PATH.exists():
        _PROMPTS = yaml.safe_load(_PROMPT_PATH.read_text(encoding="utf-8")) or {}
except Exception as exc:
    logger.warning("Failed to load prompts.yml: %s", exc)


def setup_router_node(state: PRReviewState) -> PRReviewState:
    """Initialize state and route files by language."""
    files = state.get("files", [])
    diff_content = state.get("full_diff", "")

    python_files: List[Dict[str, str]] = []
    js_files: List[Dict[str, str]] = []
    vue_files: List[Dict[str, str]] = []

    for file_data in files:
        file_path = file_data.get("file_path", "")
        lower_path = file_path.lower()
        if lower_path.endswith(".py"):
            python_files.append(file_data)
        elif lower_path.endswith(".vue"):
            vue_files.append(file_data)
        elif lower_path.endswith((".js", ".jsx", ".ts", ".tsx")):
            js_files.append(file_data)

    return {
        "python_files": python_files,
        "js_files": js_files,
        "vue_files": vue_files,
        "changed_lines_by_file": _build_changed_line_index(diff_content),
    }


def python_linter_node(state: PRReviewState) -> PRReviewState:
    findings: List[LintFinding] = []
    for file_data in state.get("python_files", []):
        findings.extend(run_ruff(file_data.get("file_path", ""), file_data.get("code_content", "")))
    return {"python_lints": findings}


def js_linter_node(state: PRReviewState) -> PRReviewState:
    findings: List[LintFinding] = []
    for file_data in state.get("js_files", []):
        findings.extend(run_eslint(file_data.get("file_path", ""), file_data.get("code_content", "")))
    return {"js_lints": findings}


def vue_linter_node(state: PRReviewState) -> PRReviewState:
    findings: List[LintFinding] = []
    for file_data in state.get("vue_files", []):
        findings.extend(run_eslint(file_data.get("file_path", ""), file_data.get("code_content", "")))
    return {"vue_lints": findings}


def security_scan_node(state: PRReviewState) -> PRReviewState:
    """Run repository-level security scanners (pip-audit + gitleaks)."""
    findings: List[LintFinding] = []
    findings.extend(run_pip_audit())
    findings.extend(run_gitleaks())
    return {"security_lints": findings}


def python_expert_node(state: PRReviewState) -> PRReviewState:
    return {"python_comments": _expert_comments_for_language(state, "python")}


def js_expert_node(state: PRReviewState) -> PRReviewState:
    return {"js_comments": _expert_comments_for_language(state, "javascript")}


def vue_expert_node(state: PRReviewState) -> PRReviewState:
    return {"vue_comments": _expert_comments_for_language(state, "vue")}


def security_expert_node(state: PRReviewState) -> PRReviewState:
    """Convert security findings to actionable inline comments."""
    return {"security_comments": _findings_to_inline_comments(state.get("security_lints", []))}


def cross_interaction_node(state: PRReviewState) -> PRReviewState:
    """Find minimal cross-file API interaction risks from full PR diff."""
    full_diff = state.get("full_diff", "")
    comments: List[InlineComment] = []

    api_route_changes = re.findall(
        r"\+\s*@(?:app|router)\.(get|post|put|delete)\(['\"]([^'\"]+)",
        full_diff,
    )
    frontend_fetches = re.findall(
        r"\+.*(?:fetch|axios\.(?:get|post|put|delete))\(['\"]([^'\"]+)",
        full_diff,
    )

    changed_routes = {_normalize_route(route) for _, route in api_route_changes}
    if changed_routes:
        for path in frontend_fetches:
            normalized_path = _normalize_route(path)
            if normalized_path.startswith("/") and normalized_path not in changed_routes:
                changed_route_text = ", ".join(sorted(changed_routes))
                comments.append(
                    {
                        "file_path": "PR",
                        "line_number": 1,
                        "severity": "medium",
                        "body": (
                            f"ðŸŒ FE-BE ìƒí˜¸ìž‘ìš© ì ê²€: í”„ë¡ íŠ¸ í˜¸ì¶œ ê²½ë¡œ `{normalized_path}` "
                            f"ê°€ ë°±ì—”ë“œ ë³€ê²½ ë¼ìš°íŠ¸({changed_route_text})ì™€ ë¶ˆì¼ì¹˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
                        ),
                    }
                )

    # method-level minimal signal when axios method is explicitly changed
    changed_backend_methods = {method.lower() for method, _ in api_route_changes}
    changed_frontend_methods = {
        method.lower()
        for method in re.findall(r"\+.*axios\.(get|post|put|delete)\(", full_diff)
    }
    missing_methods = changed_frontend_methods - changed_backend_methods
    if missing_methods and changed_backend_methods:
        missing_text = ", ".join(sorted(missing_methods))
        backend_text = ", ".join(sorted(changed_backend_methods))
        comments.append(
            {
                "file_path": "PR",
                "line_number": 1,
                "severity": "medium",
                "body": (
                    "ðŸŒ FE-BE ë©”ì„œë“œ ì ê²€(ì¤‘ê°„): í”„ë¡ íŠ¸ì—ì„œ ë³€ê²½ëœ HTTP ë©”ì„œë“œ "
                    f"`{missing_text}` ê°€ ë°±ì—”ë“œ ë³€ê²½ ë©”ì„œë“œ({backend_text})ì™€ ë‹¤ë¥¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
                ),
            }
        )

    return {"cross_interaction_comments": comments}


def supervisor_node(state: PRReviewState) -> PRReviewState:
    merged: List[InlineComment] = []
    merged.extend(state.get("python_comments", []))
    merged.extend(state.get("js_comments", []))
    merged.extend(state.get("vue_comments", []))
    merged.extend(state.get("security_comments", []))
    merged.extend(state.get("cross_interaction_comments", []))
    deduped = _dedupe_comments(merged)
    prioritized = _prioritize_comments(deduped)

    summary = _generate_summary_with_llm(state, prioritized)
    if not summary:
        grouped = _group_comments_by_source(prioritized)
        summary = {
            "positive_feedback": "ì¢‹ì€ ì‹œë„ë¥¼ ë§Žì´ í•´ì£¼ì…¨ìŠµë‹ˆë‹¤. ë‹¤ìŒ ê°œì„  í¬ì¸íŠ¸ë¥¼ ë°˜ì˜í•˜ë©´ ì½”ë“œ í’ˆì§ˆì´ ë” ë¹ ë¥´ê²Œ ì„±ìž¥í•  ìˆ˜ ìžˆì–´ìš”.",
            "highlights": [
                f"ì´ ì¸ë¼ì¸ ì½”ë©˜íŠ¸: {len(prioritized)}ê°œ",
                f"Python lint: {len(state.get('python_lints', []))}ê±´",
                f"JS lint: {len(state.get('js_lints', []))}ê±´",
                f"Vue lint: {len(state.get('vue_lints', []))}ê±´",
                f"Security scan: {len(state.get('security_lints', []))}ê±´",
                f"ë³´ì•ˆ ê´€ë ¨ ì½”ë©˜íŠ¸: {len(grouped.get('security', []))}ê°œ",
                f"ìƒí˜¸ìž‘ìš© ê´€ë ¨ ì½”ë©˜íŠ¸: {len(grouped.get('cross', []))}ê°œ",
            ],
            "top_priorities": [c["body"][:120] for c in prioritized[:3]],
            "growth_suggestions": ["ì¤‘ìš” ì´ìŠˆë¶€í„° ìˆœì„œëŒ€ë¡œ í•´ê²°í•˜ë©° ë¦¬íŒ©í† ë§ ê·¼ê±°(Why)ë¥¼ ê¸°ë¡í•´ë³´ì„¸ìš”."],
        }

    return {"overall_summary": summary, "inline_comments": prioritized}


def _expert_comments_for_language(state: PRReviewState, language: str) -> List[InlineComment]:
    if language == "python":
        files = state.get("python_files", [])
        findings = state.get("python_lints", [])
        prompt_key = "comprehensive"
    elif language == "vue":
        files = state.get("vue_files", [])
        findings = state.get("vue_lints", [])
        prompt_key = "vue_comprehensive"
    else:
        files = state.get("js_files", [])
        findings = state.get("js_lints", [])
        prompt_key = "javascript_comprehensive"

    if not files or not findings:
        return []

    filtered_findings = _filter_findings_to_changed_lines(findings, state.get("changed_lines_by_file", {}))
    if not filtered_findings:
        return []

    if os.getenv("OPENAI_API_KEY"):
        try:
            llm = CodeReviewChatOpenAI(model_name=os.getenv("OPENAI_MODEL_NAME", "gpt-4o"), temperature=0.1)
            return _llm_generate_comments(llm, prompt_key, files, filtered_findings)
        except Exception as exc:
            logger.warning("LLM expert node failed for %s: %s", language, exc)

    return _findings_to_inline_comments(filtered_findings)


def _llm_generate_comments(
    llm: CodeReviewChatOpenAI,
    prompt_key: str,
    files: List[Dict[str, str]],
    findings: List[LintFinding],
) -> List[InlineComment]:
    base_prompt = _get_review_prompt(prompt_key)
    system_content = (
        f"{base_prompt}\n\n"
        "ì•„ëž˜ ì§€ì‹œë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:\n"
        "- Linter findingsì— ì—†ëŠ” ë¬¸ì œë¥¼ ì¶”ì¸¡ìœ¼ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”(í™˜ê° ê¸ˆì§€).\n"
        "- ì‚¬ì‹¤ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ findings ë˜ëŠ” ì œê³µëœ code excerptì—ì„œë§Œ ì¸ìš©í•˜ì„¸ìš”.\n"
        "- ì‘ë‹µì€ JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ê° í•­ëª©ì€ file_path, line_number, severity, bodyë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
        "- bodyì—ëŠ” ë°˜ë“œì‹œ 'ì¹­ì°¬ í•œ ì¤„ + Why + ê°œì„  ì œì•ˆ'ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
        "- line_numberëŠ” ì •ìˆ˜ì—¬ì•¼ í•˜ë©° ë¹„ì–´ìžˆìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤."
    )

    file_excerpts = []
    for item in files:
        file_excerpts.append(
            {
                "file_path": item.get("file_path", ""),
                "code_excerpt": (item.get("code_content", "")[:600]),
            }
        )

    response = llm.invoke(
        [
            SystemMessage(content=system_content),
            HumanMessage(content=json.dumps({"files": file_excerpts[:20], "findings": findings[:30]}, ensure_ascii=False)),
        ]
    )

    payload = response.content if hasattr(response, "content") else "[]"
    data = _safe_json_load(payload)
    if not isinstance(data, list):
        return _findings_to_inline_comments(findings)

    comments: List[InlineComment] = []
    for item in data:
        if not isinstance(item, dict):
            continue
        file_path = str(item.get("file_path", "")).strip()
        line_number = item.get("line_number")
        if not file_path or not isinstance(line_number, int):
            continue
        comments.append(
            {
                "file_path": file_path,
                "line_number": line_number,
                "severity": str(item.get("severity", "medium")),
                "body": str(item.get("body", "ì¢‹ì€ ì‹œë„ìž…ë‹ˆë‹¤. Whyì™€ í•¨ê»˜ ê°œì„ ì•ˆì„ ì œì•ˆí•©ë‹ˆë‹¤.")),
            }
        )

    return comments if comments else _findings_to_inline_comments(findings)


def _generate_summary_with_llm(state: PRReviewState, comments: List[InlineComment]) -> Dict[str, Any]:
    if not os.getenv("OPENAI_API_KEY"):
        return {}

    try:
        llm = CodeReviewChatOpenAI(model_name=os.getenv("OPENAI_MODEL_NAME", "gpt-4o"), temperature=0.1)
        prompt = _get_review_prompt("pr_summary")
        system_content = (
            f"{prompt}\n\n"
            "ì¶”ê°€ ì§€ì‹œ:\n"
            "- ì‹œë‹ˆì–´ ë©˜í† ê°€ ì£¼ë‹ˆì–´ì—ê²Œ ì „í•˜ëŠ” ê¸ì •ì /ê²©ë ¤ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n"
            "- ì·¨ì•½ì /ê°œì„ ì ì€ ì„±ìž¥ í¬ì¸íŠ¸ë¡œ í‘œí˜„í•˜ì„¸ìš”.\n"
            "- ì‚¬ì‹¤ì€ commentsì™€ lint countsì—ì„œë§Œ ì¶”ì¶œí•˜ì„¸ìš”(í™˜ê° ê¸ˆì§€).\n"
            "- JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."
        )
        response = llm.invoke(
            [
                SystemMessage(content=system_content),
                HumanMessage(
                    content=json.dumps(
                        {
                            "lint_counts": {
                                "python": len(state.get("python_lints", [])),
                                "js": len(state.get("js_lints", [])),
                                "vue": len(state.get("vue_lints", [])),
                                "security": len(state.get("security_lints", [])),
                            },
                            "comments": comments[:20],
                        },
                        ensure_ascii=False,
                    )
                ),
            ]
        )
        parsed = _safe_json_load(response.content if hasattr(response, "content") else "{}")
        if isinstance(parsed, dict) and parsed.get("positive_feedback"):
            return parsed
    except Exception as exc:
        logger.warning("summary llm generation failed: %s", exc)

    return {}


def _findings_to_inline_comments(findings: List[LintFinding]) -> List[InlineComment]:
    comments: List[InlineComment] = []
    for finding in findings:
        comments.append(
            {
                "file_path": finding.get("file_path", ""),
                "line_number": int(finding.get("line", 1) or 1),
                "severity": finding.get("severity", "medium"),
                "body": (
                    f"[ì¹­ì°¬] ì¢‹ì€ ì‹œë„ìž…ë‹ˆë‹¤. [{finding.get('source', 'linter')}] {finding.get('message', 'ì´ìŠˆ í™•ì¸ í•„ìš”')} "
                    f"(rule: {finding.get('rule_id', 'N/A')}) / Why: ì•ˆì •ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë†’ì¼ ìˆ˜ ìžˆì–´ìš”."
                ),
            }
        )
    return comments


def _build_changed_line_index(diff_content: str) -> Dict[str, List[int]]:
    if not diff_content:
        return {}

    parser = DiffParser()
    file_diffs = parser.parse_diff(diff_content)

    index: Dict[str, List[int]] = defaultdict(list)
    for file_diff in file_diffs:
        for hunk in file_diff.hunks:
            for line_no, _ in hunk.additions:
                index[file_diff.new_path].append(line_no)

    return {k: sorted(set(v)) for k, v in index.items()}


def _filter_findings_to_changed_lines(
    findings: List[LintFinding],
    changed_lines_by_file: Dict[str, List[int]],
) -> List[LintFinding]:
    if not changed_lines_by_file:
        return findings

    filtered: List[LintFinding] = []
    for finding in findings:
        file_path = finding.get("file_path", "")
        changed_lines = set(changed_lines_by_file.get(file_path, []))
        if not changed_lines:
            continue
        line = int(finding.get("line", 1) or 1)
        end_line = int(finding.get("end_line", line) or line)
        if any(current_line in changed_lines for current_line in range(line, end_line + 1)):
            filtered.append(finding)
    return filtered


def _dedupe_comments(comments: List[InlineComment]) -> List[InlineComment]:
    seen = set()
    deduped: List[InlineComment] = []
    for comment in comments:
        key = (comment.get("file_path", ""), comment.get("line_number", 1), comment.get("body", ""))
        if key in seen:
            continue
        seen.add(key)
        deduped.append(comment)
    return deduped


def _prioritize_comments(comments: List[InlineComment]) -> List[InlineComment]:
    """Sort comments by severity then source priority for actionable ordering."""

    severity_weight = {"high": 0, "critical": 0, "medium": 1, "low": 2}

    def _source_bucket(comment: InlineComment) -> int:
        body = str(comment.get("body", "")).lower()
        if "pip-audit" in body or "gitleaks" in body:
            return 0
        if "fe-be" in body or "ìƒí˜¸ìž‘ìš©" in body or "ë©”ì„œë“œ ì ê²€" in body:
            return 1
        return 2

    return sorted(
        comments,
        key=lambda c: (
            severity_weight.get(str(c.get("severity", "medium")).lower(), 1),
            _source_bucket(c),
            str(c.get("file_path", "")),
            int(c.get("line_number", 1) or 1),
        ),
    )


def _group_comments_by_source(comments: List[InlineComment]) -> Dict[str, List[InlineComment]]:
    grouped: Dict[str, List[InlineComment]] = {"security": [], "cross": [], "lint": []}
    for comment in comments:
        body = str(comment.get("body", "")).lower()
        if "pip-audit" in body or "gitleaks" in body:
            grouped["security"].append(comment)
        elif "fe-be" in body or "ìƒí˜¸ìž‘ìš©" in body or "ë©”ì„œë“œ ì ê²€" in body:
            grouped["cross"].append(comment)
        else:
            grouped["lint"].append(comment)
    return grouped


def _get_review_prompt(style_key: str) -> str:
    review_styles = _PROMPTS.get("review_styles", {}) if isinstance(_PROMPTS, dict) else {}
    style = review_styles.get(style_key, {}) if isinstance(review_styles, dict) else {}
    return style.get("system_prompt", "ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ë©˜í†  ì½”ë“œë¦¬ë·°ì–´ìž…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.")


def _safe_json_load(payload: str) -> Any:
    text = (payload or "").strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        match = re.search(r"(\{.*\}|\[.*\])", text, re.DOTALL)
        if not match:
            return {}
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            return {}


def _normalize_route(path: str) -> str:
    """Normalize route for lightweight FE/BE route comparison."""
    normalized = (path or "").strip()
    if not normalized:
        return normalized
    # remove query string and trailing slash (except root)
    normalized = normalized.split("?", 1)[0]
    if normalized != "/":
        normalized = normalized.rstrip("/")
    return normalized
